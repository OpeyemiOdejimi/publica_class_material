{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 1: Introduction to LangGraph & State Management\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand what LangGraph is and why we need it\n",
    "- Learn core concepts: nodes, edges, state, checkpointers\n",
    "- Build a simple stateful chatbot with conversation memory\n",
    "- Compare LangGraph with LangChain chains\n",
    "\n",
    "**Prerequisites:** RAG with LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: What is LangGraph?\n",
    "\n",
    "### The Problem with Chains\n",
    "\n",
    "You have learned about **LangChain chains** - they're great for fixed pipelines:\n",
    "\n",
    "```python\n",
    "# Always follows this path:\n",
    "chain = retriever | prompt | llm | parser\n",
    "```\n",
    "\n",
    "But what if you need:\n",
    "- **Decisions during execution?** (\"Should I retrieve or not?\")\n",
    "- **Loops and cycles?** (\"Try again if answer is poor\")\n",
    "- **Multiple tools?** (\"Use search OR calculator OR retrieval\")\n",
    "- **Complex control flow?** (\"If X then Y, else Z\")\n",
    "\n",
    "**That's where LangGraph comes in!**\n",
    "\n",
    "### LangGraph = State Machines for Agents\n",
    "\n",
    "LangGraph lets you build **graphs** where:\n",
    "- **Nodes** are functions that process state\n",
    "- **Edges** connect nodes (fixed or conditional)\n",
    "- **State** flows through the graph\n",
    "- **Agents make decisions** about which path to take\n",
    "\n",
    "```\n",
    "           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  START ‚îÄ‚îÄ‚ñ∂‚îÇ  Node 1  ‚îÇ\n",
    "           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                ‚îÇ\n",
    "                ‚ñº\n",
    "           Decision?\n",
    "           ‚îú‚îÄ YES ‚îÄ‚ñ∂ Node 2 ‚îÄ‚îÄ‚îê\n",
    "           ‚îî‚îÄ NO  ‚îÄ‚ñ∂ Node 3 ‚îÄ‚îÄ‚î§\n",
    "                               ‚ñº\n",
    "                              END\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LangGraph Application Architecture](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/LangGraph/application.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q langgraph langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up OpenAI API Key\n",
    "\n",
    "Create a `.env` file in your project directory with:\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found! Please set it in your .env file.\")\n",
    "\n",
    "print(\"‚úÖ API key loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the LLM\n",
    "\n",
    "We'll use **GPT-4o-mini** - it's fast and cost-effective for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Core Concept #1 - State\n",
    "\n",
    "**State** ‚ÄúThe single source of truth for the whole agent execution.‚Äù  \n",
    "**State** is the data that flows through your graph.\n",
    "\n",
    "### MessagesState\n",
    "\n",
    "For chatbots, LangGraph provides `MessagesState` - it stores conversation history:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Hello\"),\n",
    "        AIMessage(content=\"Hi! How can I help?\"),\n",
    "        HumanMessage(content=\"What's Python?\"),\n",
    "        # ... more messages\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "This is similar to `ConversationBufferMemory`, but more flexible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ü§î Reflection Question:** \n",
    "How is this different from ConversationBufferMemory? In chains, memory was managed separately. In LangGraph, it's part of the state that flows through nodes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Core Concept #2 - Nodes\n",
    "\n",
    "**Nodes** are functions that process state and return updates.\n",
    "\n",
    "### Node Function Signature\n",
    "\n",
    "```python\n",
    "def my_node(state: MessagesState) -> dict:\n",
    "    # Process state\n",
    "    # Return updates to state\n",
    "    return {\"messages\": [new_message]}\n",
    "```\n",
    "\n",
    "### The Assistant Node\n",
    "\n",
    "Let's create our first node - it sends messages to the LLM and gets a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Assistant node defined\n"
     ]
    }
   ],
   "source": [
    "# System prompt that defines assistant behavior\n",
    "sys_msg = SystemMessage(\n",
    "    content=\"You are a friendly assistant that answers user questions. Be helpful and concise.\"\n",
    ")\n",
    "\n",
    "def assistant(state: MessagesState) -> dict:\n",
    "    \"\"\"\n",
    "    The assistant node - processes messages and generates response.\n",
    "    \"\"\"\n",
    "    # Combine system prompt with conversation history\n",
    "    messages = [sys_msg] + state[\"messages\"]\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Return as state update\n",
    "    return {\"messages\": [AIMessage(content=response.content)]}\n",
    "\n",
    "print(\"‚úÖ Assistant node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Key Point:** The node doesn't modify state directly - it returns updates that LangGraph applies automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a Retriever Node\n",
    "You don't need to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_docs(state: MessagesState):\n",
    "#     query = state[\"messages\"][-1].content  # latest HumanMessage\n",
    "#     docs = retriever.invoke(query)\n",
    "\n",
    "#     return {\n",
    "#         \"messages\": [\n",
    "#             ToolMessage(\n",
    "#                 content=\"\\n\".join(d.page_content for d in docs),\n",
    "#                 name=\"retriever\"\n",
    "#             )\n",
    "#         ]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Core Concept 2B - Edges\n",
    "\n",
    "**Edges** are the connections between nodes that control the flow of your agent.\n",
    "\n",
    "Think of edges as **roads** between cities (nodes). They determine which node to visit next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Types of Edges\n",
    "\n",
    "LangGraph has two types of edges:\n",
    "\n",
    "1. **Fixed/Static Edges** (Normal Edges)\n",
    "   - Always go from Node A to Node B\n",
    "   - No decision-making\n",
    "   - Used for predictable flows\n",
    "\n",
    "2. **Conditional Edges**\n",
    "   - Decide which node to visit next based on state\n",
    "   - Enable agent decision-making\n",
    "   - Used for dynamic, intelligent behavior\n",
    "\n",
    "```\n",
    "Fixed/Static Edge:\n",
    "  Node A ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Node B  (always goes to B)\n",
    "\n",
    "Conditional Edge:\n",
    "  Node A ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Decision?\n",
    "                    ‚îú‚îÄ Condition 1 ‚îÄ‚îÄ‚ñ∂ Node B\n",
    "                    ‚îú‚îÄ Condition 2 ‚îÄ‚îÄ‚ñ∂ Node C\n",
    "                    ‚îî‚îÄ Condition 3 ‚îÄ‚îÄ‚ñ∂ Node D\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Core Concept #3 - Building the Graph\n",
    "\n",
    "Now let's connect everything into a **StateGraph**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graph structure defined\n"
     ]
    }
   ],
   "source": [
    "# Create a StateGraph with MessagesState\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add the assistant node\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "\n",
    "# Define the flow:\n",
    "# START ‚Üí assistant ‚Üí END\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_edge(\"assistant\", END)\n",
    "\n",
    "print(\"‚úÖ Graph structure defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Flow\n",
    "\n",
    "```\n",
    "START ‚Üí [assistant node] ‚Üí END\n",
    "```\n",
    "\n",
    "- **START:** Entry point (receives user message)\n",
    "- **assistant:** Processes message and generates response\n",
    "- **END:** Exit point (returns final state)\n",
    "\n",
    "This is simple now, but later we'll add conditional edges for agentic behavior!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Core Concept #4 - Checkpointers (Memory)\n",
    "\n",
    "**Checkpointers** save state between interactions - this gives our agent memory!\n",
    "\n",
    "Without checkpointer:\n",
    "- Each call starts fresh\n",
    "- No conversation history\n",
    "- Agent forgets everything\n",
    "\n",
    "With checkpointer:\n",
    "- State persists between calls\n",
    "- Agent remembers conversation\n",
    "- Multi-turn conversations work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent compiled with memory\n"
     ]
    }
   ],
   "source": [
    "# Create a memory checkpointer (stores in memory)\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the graph WITH memory\n",
    "agent = builder.compile(checkpointer=memory)\n",
    "\n",
    "print(\"‚úÖ Agent compiled with memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîß Production Note:** `MemorySaver` stores in RAM (lost on restart). For production, use:\n",
    "- `SqliteSaver` - persists to SQLite database\n",
    "- `MongoDBSaver` - persists to MongoDB\n",
    "\n",
    "We'll use MemorySaver for learning since it's simple!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Visualizing the Graph\n",
    "\n",
    "One of LangGraph's best features - **visual representation** of your agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the graph structure\n",
    "try:\n",
    "    display(Image(agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")\n",
    "    print(\"Graph structure: START ‚Üí assistant ‚Üí END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Pro Tip:** Always visualize your graph! It helps you understand and debug agent behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Running the Agent\n",
    "\n",
    "Now let's actually use our agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session IDs (Thread IDs)\n",
    "\n",
    "Each conversation has a unique **thread_id**. Messages with the same thread_id share memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting conversation with session ID: chat-session-0012\n"
     ]
    }
   ],
   "source": [
    "# Define a session ID for this conversation\n",
    "session_id = \"chat-session-0012\"\n",
    "\n",
    "print(f\"Starting conversation with session ID: {session_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function for Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conversation function ready\n"
     ]
    }
   ],
   "source": [
    "def run_conversation(user_input: str, thread_id: str = session_id):\n",
    "    \"\"\"\n",
    "    Send a message to the agent and get response.\n",
    "    ‚ö†Ô∏è WARNING: Using default thread_id shares conversation acrosss all calls!\n",
    "    In production, ALWAYS provide unique thread_id per user.\n",
    "    \"\"\"\n",
    "    # Invoke the agent\n",
    "    result = agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "    \n",
    "    # Print the conversation\n",
    "    for message in result[\"messages\"]:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            print(f\"\\nüë§ User: {message.content}\")\n",
    "        elif isinstance(message, AIMessage):\n",
    "            print(f\"ü§ñ Agent: {message.content}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"‚úÖ Conversation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Single Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I'm an AI assistant, and I don't have a specific name. You can just call me Assistant. How can I help you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "run_conversation(\"Hello! What's your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Multi-Turn Conversation (Memory Test!)\n",
    "\n",
    "Now let's test if the agent remembers context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I'm an AI assistant, and I don't have a specific name. You can just call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's great! Blue is a calming and versatile color. Do you have a favorite shade of blue, or do you like it in general?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# First message\n",
    "run_conversation(\"My favorite color is blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I'm an AI assistant, and I don't have a specific name. You can just call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's great! Blue is a calming and versatile color. Do you have a favorite shade of blue, or do you like it in general?\n",
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: Your favorite color is blue! Would you like to share more about why you like it?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question - does it remember?\n",
    "run_conversation(\"What's my favorite color?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: I'm not sure what your favorite color is! If you'd like to share it with me, I'd be happy to know!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "run_conversation(\"What's my favorite color?\", thread_id=\"111\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ Success!** The agent remembered your favorite color because:\n",
    "1. The checkpointer saved the state after the first message\n",
    "2. The same thread_id retrieved that saved state\n",
    "3. The conversation history was passed to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Context-Dependent Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I'm an AI assistant, and I don't have a specific name. You can just call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's great! Blue is a calming and versatile color. Do you have a favorite shade of blue, or do you like it in general?\n",
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: Your favorite color is blue! Would you like to share more about why you like it?\n",
      "\n",
      "üë§ User: I'm learning about RAG systems\n",
      "ü§ñ Agent: That's interesting! RAG (Retrieval-Augmented Generation) systems combine retrieval of information from external sources with generation capabilities to provide more accurate and contextually relevant responses. They typically involve a two-step process: first retrieving relevant documents or data from a database, and then using that information to generate a coherent response. \n",
      "\n",
      "Do you have specific questions about RAG systems, or is there a particular aspect you're interested in?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Start a new topic\n",
    "run_conversation(\"I'm learning about RAG systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I'm an AI assistant, and I don't have a specific name. You can just call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's great! Blue is a calming and versatile color. Do you have a favorite shade of blue, or do you like it in general?\n",
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: Your favorite color is blue! Would you like to share more about why you like it?\n",
      "\n",
      "üë§ User: I'm learning about RAG systems\n",
      "ü§ñ Agent: That's interesting! RAG (Retrieval-Augmented Generation) systems combine retrieval of information from external sources with generation capabilities to provide more accurate and contextually relevant responses. They typically involve a two-step process: first retrieving relevant documents or data from a database, and then using that information to generate a coherent response. \n",
      "\n",
      "Do you have specific questions about RAG systems, or is there a particular aspect you're interested in?\n",
      "\n",
      "üë§ User: Can you explain the main components?\n",
      "ü§ñ Agent: Certainly! The main components of a Retrieval-Augmented Generation (RAG) system typically include:\n",
      "\n",
      "1. **Retrieval Component**: \n",
      "   - This part is responsible for fetching relevant documents or pieces of information from a knowledge base or database. It often uses techniques like vector embeddings, TF-IDF, or traditional search algorithms to match queries with relevant content.\n",
      "\n",
      "2. **Generation Component**: \n",
      "   - After retrieving relevant information, this component generates a coherent response. It usually involves a language model (like GPT or BERT) that takes the retrieved information as context to produce natural language text.\n",
      "\n",
      "3. **Combiner/Integrator**: \n",
      "   - This component integrates the retrieved information with the generative model's capabilities to ensure that the generated response is contextually accurate and relevant to the user's query.\n",
      "\n",
      "4. **User Input Interface**: \n",
      "   - This is the front-end part where users interact with the system, submitting queries and receiving responses.\n",
      "\n",
      "5. **Feedback Loop**: \n",
      "   - Some RAG systems include mechanisms for learning from user interactions, allowing them to improve retrieval and generation over time based on feedback.\n",
      "\n",
      "If you have more specific aspects or components you'd like to know about, feel free to ask!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Reference it\n",
    "run_conversation(\"Can you explain the main components?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ User: Hello! What's your name?\n",
      "ü§ñ Agent: Hello! I'm an AI assistant, and I don't have a specific name. You can just call me Assistant. How can I help you today?\n",
      "\n",
      "üë§ User: My favorite color is blue\n",
      "ü§ñ Agent: That's great! Blue is a calming and versatile color. Do you have a favorite shade of blue, or do you like it in general?\n",
      "\n",
      "üë§ User: What's my favorite color?\n",
      "ü§ñ Agent: Your favorite color is blue! Would you like to share more about why you like it?\n",
      "\n",
      "üë§ User: I'm learning about RAG systems\n",
      "ü§ñ Agent: That's interesting! RAG (Retrieval-Augmented Generation) systems combine retrieval of information from external sources with generation capabilities to provide more accurate and contextually relevant responses. They typically involve a two-step process: first retrieving relevant documents or data from a database, and then using that information to generate a coherent response. \n",
      "\n",
      "Do you have specific questions about RAG systems, or is there a particular aspect you're interested in?\n",
      "\n",
      "üë§ User: Can you explain the main components?\n",
      "ü§ñ Agent: Certainly! The main components of a Retrieval-Augmented Generation (RAG) system typically include:\n",
      "\n",
      "1. **Retrieval Component**: \n",
      "   - This part is responsible for fetching relevant documents or pieces of information from a knowledge base or database. It often uses techniques like vector embeddings, TF-IDF, or traditional search algorithms to match queries with relevant content.\n",
      "\n",
      "2. **Generation Component**: \n",
      "   - After retrieving relevant information, this component generates a coherent response. It usually involves a language model (like GPT or BERT) that takes the retrieved information as context to produce natural language text.\n",
      "\n",
      "3. **Combiner/Integrator**: \n",
      "   - This component integrates the retrieved information with the generative model's capabilities to ensure that the generated response is contextually accurate and relevant to the user's query.\n",
      "\n",
      "4. **User Input Interface**: \n",
      "   - This is the front-end part where users interact with the system, submitting queries and receiving responses.\n",
      "\n",
      "5. **Feedback Loop**: \n",
      "   - Some RAG systems include mechanisms for learning from user interactions, allowing them to improve retrieval and generation over time based on feedback.\n",
      "\n",
      "If you have more specific aspects or components you'd like to know about, feel free to ask!\n",
      "\n",
      "üë§ User: Which component is most important?\n",
      "ü§ñ Agent: While all components of a Retrieval-Augmented Generation (RAG) system are important for its overall effectiveness, the most critical component often depends on the specific use case. However, generally speaking:\n",
      "\n",
      "1. **Retrieval Component**: This is often considered the most crucial because the quality and relevance of the retrieved information directly impact the accuracy and relevance of the generated responses. If the retrieval system fails to find pertinent documents, the generative model will have insufficient context to produce a meaningful answer.\n",
      "\n",
      "2. **Generation Component**: This is also vital, as a powerful language model can produce high-quality text. If the generation process doesn't effectively utilize the retrieved information, the output may lack coherence or relevance.\n",
      "\n",
      "In summary, while both the retrieval and generation components are essential, the retrieval component is typically seen as foundational because it sets the stage for the quality of the generated response. The effectiveness of a RAG system hinges on how well it retrieves relevant data.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question\n",
    "run_conversation(\"Which component is most important?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Notice:** The agent maintains context across multiple turns - just like a real conversation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Multiple Conversations (Different Thread IDs)\n",
    "\n",
    "Let's test that different thread IDs have separate memories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîµ CONVERSATION 1\n",
      "\n",
      "üë§ User: My name is Alice\n",
      "ü§ñ Agent: Nice to meet you, Alice! How can I assist you today?\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üü¢ CONVERSATION 2\n",
      "\n",
      "üë§ User: My name is Bob\n",
      "ü§ñ Agent: Hi Bob! How can I assist you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Conversation 1\n",
    "print(\"\\nüîµ CONVERSATION 1\")\n",
    "run_conversation(\"My name is Alice\", thread_id=\"user_alicee\")\n",
    "\n",
    "# Conversation 2 (different user)\n",
    "print(\"\\nüü¢ CONVERSATION 2\")\n",
    "run_conversation(\"My name is Bob\", thread_id=\"user_bobb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîµ BACK TO CONVERSATION 1\n",
      "\n",
      "üë§ User: My name is Alice\n",
      "ü§ñ Agent: Nice to meet you, Alice! How can I assist you today?\n",
      "\n",
      "üë§ User: What's my name?\n",
      "ü§ñ Agent: Your name is Alice! How can I help you today?\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Back to Alice - does it remember her name?\n",
    "print(\"\\nüîµ BACK TO CONVERSATION 1\")\n",
    "run_conversation(\"What's my name?\", thread_id=\"user_alicee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ BACK TO CONVERSATION 2\n",
      "\n",
      "üë§ User: My name is Bob\n",
      "ü§ñ Agent: Hi Bob! How can I assist you today?\n",
      "\n",
      "üë§ User: What's my name?\n",
      "ü§ñ Agent: Your name is Bob!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Back to Bob\n",
    "print(\"\\nüü¢ BACK TO CONVERSATION 2\")\n",
    "run_conversation(\"What's my name?\", thread_id=\"user_bobb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéØ Key Insight:** Each thread_id maintains its own conversation history. This is how you'd handle multiple users in production!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Interactive Chat Loop\n",
    "\n",
    "Let's create an interactive chat session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ü§ñ Interactive Chat Started\n",
      "Type your message and press Enter. Type 'exit' to quit.\n",
      "======================================================================\n",
      "\n",
      "\n",
      "ü§ñ Agent: Hello! How can I assist you today?\n",
      "\n",
      "ü§ñ Agent: As of my last update in October 2023, the governor of Ogun State, Nigeria, is Dapo Abiodun. He has been in office since May 29, 2019. However, please verify with current sources, as this information may change.\n",
      "\n",
      "ü§ñ Agent: You're welcome! If you have any more questions, feel free to ask.\n",
      "\n",
      "üëã Goodbye!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Run an interactive chat session.\n",
    "    Type 'exit' or 'quit' to stop.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ü§ñ Interactive Chat Started\")\n",
    "    print(\"Type your message and press Enter. Type 'exit' to quit.\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    thread_id = \"interactive_session2\"\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nüë§ You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"\\nüëã Goodbye!\\n\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        # Get response\n",
    "        result = agent.invoke(\n",
    "            {\"messages\": [HumanMessage(content=user_input)]},\n",
    "            config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "        )\n",
    "        \n",
    "        # Print agent's response\n",
    "        agent_message = result[\"messages\"][-1]\n",
    "        print(f\"\\nü§ñ Agent: {agent_message.content}\")\n",
    "\n",
    "# Uncomment to run interactive chat:\n",
    "interactive_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 11: ConversationalRetrievalChain\n",
    "\n",
    "Let's compare LangGraph with the memory you learned in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationalRetrievalChain\n",
    "\n",
    "```python\n",
    "# Langchain approach\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"question\": \"What is Python?\"})\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- ‚úÖ Simple to use\n",
    "- ‚úÖ Built-in memory\n",
    "- ‚ùå Fixed pipeline (always retrieves)\n",
    "- ‚ùå No conditional logic\n",
    "- ‚ùå Can't add complex decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 12: LangGraph Agent\n",
    "\n",
    "```python\n",
    "# LangGraph approach\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"assistant\", assistant_node)\n",
    "memory = MemorySaver()\n",
    "agent = builder.compile(checkpointer=memory)\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is Python?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"user_123\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "**Characteristics:**\n",
    "- ‚úÖ Flexible - add any nodes/edges\n",
    "- ‚úÖ Conditional logic (coming in Topic 2)\n",
    "- ‚úÖ Agents can make decisions\n",
    "- ‚úÖ Supports cycles and loops\n",
    "- ‚ùå More complex to set up\n",
    "- ‚ùå Requires understanding graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use What?\n",
    "\n",
    "| Use Case | LangChains | LangGraph |\n",
    "|----------|-------------------|------------------------|\n",
    "| Simple chatbot | ‚úÖ Perfect | ‚ö†Ô∏è Overkill |\n",
    "| Fixed RAG pipeline | ‚úÖ Great | ‚ö†Ô∏è Unnecessary |\n",
    "| Agent with tools | ‚ùå Limited | ‚úÖ Ideal |\n",
    "| Conditional retrieval | ‚ùå Can't do | ‚úÖ Perfect |\n",
    "| Multi-agent systems | ‚ùå Not possible | ‚úÖ Built for it |\n",
    "\n",
    "**Rule of thumb:** If you need decision-making during execution, use LangGraph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 12: Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **LangGraph Basics**\n",
    "   - LangGraph enables agentic behavior through graphs\n",
    "   - Better than chains when you need decisions during execution\n",
    "\n",
    "2. **Core Concepts**\n",
    "   - **State:** Data flowing through the graph (MessagesState for chat)\n",
    "   - **Nodes:** Functions that process and update state\n",
    "   - **Edges:** Connections between nodes (fixed or conditional)\n",
    "   - **Checkpointers:** Persist state for memory across sessions\n",
    "\n",
    "3. **Practical Skills**\n",
    "   - Built a stateful chatbot\n",
    "   - Used thread_id for multi-user conversations\n",
    "   - Visualized graph structure\n",
    "   - Compared with Module 9 chains\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Topic 2: Tool Integration**\n",
    "- Add tools for agents (search, calculator, retrieval)\n",
    "- Conditional edges (agent decides which tool to use)\n",
    "- This is where LangGraph really shines!\n",
    "\n",
    "**Topic 3: Agentic RAG**\n",
    "- Agent that decides when to retrieve\n",
    "- Combines everything from Topics 1-2\n",
    "- The core concept of this module!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practice Exercises\n",
    "## Exercise 1: Build Your First Stateful Agent\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "**Estimated Time:** 30-45 minutes\n",
    "\n",
    "### Task\n",
    "Build a simple customer support chatbot that remembers conversation context.\n",
    "\n",
    "### Requirements\n",
    "1. Create a StateGraph with MessagesState\n",
    "2. Add a system prompt that makes the agent act as a helpful customer support rep\n",
    "3. Use MemorySaver checkpointer for memory\n",
    "4. Test with a multi-turn conversation where context matters\n",
    "\n",
    "### Example Conversation\n",
    "```\n",
    "User: \"I bought a laptop last week\"\n",
    "Agent: \"I'd be happy to help with your laptop! What seems to be the issue?\"\n",
    "User: \"It won't turn on\"\n",
    "Agent: \"I understand your laptop won't turn on. Have you tried...\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to exercise 1 : Build your first stateful agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model right away, since imports have been done earlier in the code book\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(f\"LLM initialized: {llm.model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Define system prompt so agent can act as customer support representative\n",
    "\n",
    "system_prompt = SystemMessage(\n",
    "    content=(\n",
    "        \"You are a helpful and polite customer support rep. \"\n",
    "        \"You listen carefully to the problems customers have, remember previous details, \"\n",
    "        \"and respond clearly and professionally.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"System Prompt initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the brain of the chatbot\n",
    "\n",
    "def support_agent(state: MessagesState):\n",
    "    # Combine system prompt + conversation history\n",
    "    messages = [system_prompt] + state[\"messages\"]\n",
    "    \n",
    "    # Ask the LLM to respond\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [response]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " StateGraph created successfully\n"
     ]
    }
   ],
   "source": [
    "# Build stateGraph\n",
    "\n",
    "graph = StateGraph(MessagesState)\n",
    "\n",
    "# Add a single node\n",
    "graph.add_node(\"support\", support_agent)\n",
    "\n",
    "# Define flow\n",
    "graph.set_entry_point(\"support\")\n",
    "graph.add_edge(\"support\", END)\n",
    "\n",
    "\n",
    "print(f\" StateGraph created successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Enabled\n"
     ]
    }
   ],
   "source": [
    "# Enable Memory\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = graph.compile(checkpointer=memory)\n",
    "\n",
    "print(f\"Memory Enabled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"customer-1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for reaching out! Could you please provide me with more details about your purchase? For instance, what issues are you experiencing with the laptop, or is there something specific you would like assistance with?\n"
     ]
    }
   ],
   "source": [
    "# Turn 1\n",
    "\n",
    "result = app.invoke(\n",
    "    {\n",
    "        \"messages\": [(\"user\", \"I bought a laptop last week\")]\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(result[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry to hear that your laptop won't turn on. Let's try to troubleshoot the issue together. Here are a few steps you can take:\n",
      "\n",
      "1. **Check the Power Supply**: Ensure that the laptop is plugged in and that the power outlet is working. If possible, try a different outlet or use another charger.\n",
      "\n",
      "2. **Remove External Devices**: Disconnect any peripherals like USB drives, external mice, or keyboards. Sometimes, these can cause startup issues.\n",
      "\n",
      "3. **Perform a Hard Reset**: If your laptop has a removable battery, take it out and hold the power button for about 15 seconds. Then, reinsert the battery and try turning it on again. If it doesn't have a removable battery, just hold the power button for about 15 seconds.\n",
      "\n",
      "4. **Check for Indicator Lights**: Look for any lights that indicate the laptop is receiving power or attempting to start. \n",
      "\n",
      "If none of these steps work, please let me know the make and model of the laptop, and any additional details about what happens when you try to turn it on (any sounds, lights, etc.). This information will help us further diagnose the problem.\n"
     ]
    }
   ],
   "source": [
    "# Turn 2\n",
    "\n",
    "result = app.invoke(\n",
    "    {\n",
    "        \"messages\": [(\"user\", \"It won't turn on\")]\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(result[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That makes sense! It's easy to overlook the charging aspect. Make sure to connect the laptop to a power source and let it charge for a while. If the battery was completely drained, it might take a few minutes before it shows any signs of life.\n",
      "\n",
      "Once it has charged for a bit, try turning it on again. If you encounter any further issues or have any questions, feel free to reach out!\n"
     ]
    }
   ],
   "source": [
    "# Turn 3\n",
    "\n",
    "result = app.invoke(\n",
    "    {\n",
    "        \"messages\": [(\"user\", \"Oh! I did not properly charge the laptop\")]\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reflection Questions\n",
    "\n",
    "1. **How does LangGraph's state management differ from ConversationalRetrievalChain memory?**\n",
    "   \n",
    "2. **Why do we need thread_id for conversations?**\n",
    "   \n",
    "3. **What happens if you don't configure a checkpointer?**\n",
    "   \n",
    "4. **When would you choose chains over LangGraph?**\n",
    "   \n",
    "5. **How would you debug an agent that's not behaving correctly?**\n",
    "\n",
    "Write your answers below or discuss with your study group!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**üéâ Topic 1 Complete!** \n",
    "\n",
    "You now understand LangGraph fundamentals. Next up: **Tool Integration** - where agents become truly powerful!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
